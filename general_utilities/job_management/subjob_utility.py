import inspect
import os
from datetime import datetime
from enum import Enum, auto
from importlib import import_module
from typing import TypedDict, Dict, Any, List, Iterator, Optional, Callable

import dxpy
import math
from time import sleep, time

from general_utilities.association_resources import download_dxfile_by_name
from general_utilities.job_management.command_executor import build_default_command_executor, CommandExecutor
from general_utilities.mrc_logger import MRCLogger


class Environment(Enum):
    """An Enum that defines the launch environment used to generate jobs.

    A bit strange, but the Environment enum's value is a class that we can instantiate for our specific job type. This
    allows each job to be queued with it's expected instantiating class and make calling the required class more
    abstract.
    """

    DX = dxpy.DXJob
    LOCAL = dxpy.DXApplet


class DXJobInfo(TypedDict):
    """A TypedDict containing job information not provided by DNANexus, but generated by the :func:`SubjobUtility()`
    class.

    :cvar function: The string representation of the function to be used for a given subjob
    :cvar properties: Additional properties to be passed to the 'properties' parameter of :func:`dxpy.DXJob.run()`
    :cvar input: The inputs as defined in the Applet / Function specification
    :cvar outputs: The outputs as defined in the Applet / Function specification
    :cvar job_type: A :func:`Environment()` enum defining where subjobs have been launched from
    :cvar retries: Current number of retries for this job. Will be incremented whenever a job fails
    :cvar destination: The output folder for :cvar outputs:. Only used by Environment.DXApplet jobs
    :cvar name: The name for a job
    :cvar instance_type: The requested instance_type for a job. Must conform to available DNANexus instance_types.
    """
    function: str
    properties: Dict[str, str]
    input: Dict[str, Any]
    outputs: List[str]
    job_type: Environment
    retries: int
    destination: Optional[str]
    name: Optional[str]
    instance_type: Optional[str]


class DXJobDict(TypedDict):
    """A TypedDict that stores information about a job.

    :cvar job_class: An object of :func:`dxpy.DXJob()` that contains information necessary for DNANexus to find this
        job
    :cvar job_info: A :func:`DXJobInfo()` TypedDict that contains job information necessary to monitor jobs generated by
        the :func:`JobStatus()` class
    """
    job_class: dxpy.DXJob
    job_info: DXJobInfo


class RunningStatus(Enum):
    """A more succinct version of JobStatus than that defined by DNANexus.
    """

    # DO NOT remove the '()' after auto, even though pycharm says it is wrong. IT IS NOT WRONG.
    COMPLETE = auto()
    RUNNING = auto()
    FAILED = auto()


class JobStatus(Enum):
    """The purpose of this Enum is to allow easy translation of DNANexus-specific job-status into a format with
    easily-defined logic. This logic is actually defined in the RunningStatus enum.

    For more information on these types, see
    https://documentation.dnanexus.com/user/running-apps-and-workflows/job-lifecycle
    """
    DONE = RunningStatus.COMPLETE
    IDLE = RunningStatus.RUNNING
    RUNNABLE = RunningStatus.RUNNING
    RUNNING = RunningStatus.RUNNING
    WAITING_ON_OUTPUT = RunningStatus.RUNNING
    WAITING_ON_INPUT = RunningStatus.RUNNING
    TERMINATING = RunningStatus.RUNNING
    TERMINATED = RunningStatus.FAILED
    FAILED = RunningStatus.FAILED


class SubjobUtility:
    """A class that contains information on, launches, and monitors subjobs on the DNANexus platform.

    This class functions in two ways, depending on the methods used to queue jobs:

    1. If run from a local machine (e.g., a macbook) via :func:launch_applet() – Will launch new jobs that are
    not dependent on any current running job

    2. If run from a currently running DNANexus job via :func:launch_job() – Will launch subjobs that are
    dependent on the current job to be run via the DXJob class

    See individual method documentation for more information, but briefly, the workflow for using this class is
    the following:

    1. Queue jobs using either the :func:launch_applet() or :func:launch_job(). Do NOT use both with the same
    constructor (there are checks to prevent this)!

    2. Submit jobs that have been queued with :func:submit_queue(). After calling this method, the queue closes
    and new jobs can no longer be added to prevent iteration errors when collecting job output.

    3. Collect jobs using the built-in iterator (specified by the :func:__iter__() dunder method).

    A brief example follows::

        # Call this class with the default constructor
        subjob_utility = SubjobUtility()

        # Add jobs to the queue
        phenotype = 'cardiac_arrest'
        for chromosome in range(1,23):
            subjob_utility.launch_job(function=burden_interaction,
                                      inputs={'chromosome': chromosome,
                                              'pheno_name': phenotype},
                                      outputs=['outfile', 'outname'],
                                      instance_type='mem3_ssd1_v2_x8',
                                      name=f'{chromosome}_{phenotype}_subjob')

        # Launch jobs on DNANexus
        subjob_utility.submit_queue()

        # Collect outputs:
        outputs = []
        # 1st for-loop iterates over individual jobs
        for output in subjob_utility:
            # Each output is either the actual value, or if a file, a dxpy.DXLink() reference to a file OR a
            # pathlib.Path to that file on the local file system if self._download_oncomplete is True

            # 2nd for-loop iterates over outputs
            for output_key, output_value in output.items():
                print(f'Output for {output_key}: {output_value}')
                outputs.append(output_value)

            # Or, the user can also directly query the output using the name given to the launch_job method
            # outputs param:
            print(f'output for the outname value is: {output["outname"]})

    :param concurrent_job_limit: Number of jobs that can be run at once. Default of 100 is the actual limit for
        concurrent jobs on the DNANexus platform. It is also wishful in that you will rarely be able to have 100
        jobs simultaneously running. [100]
    :param retries: Number of times to retry a job before marking it as a fail. Default is intended to let jobs
        interrupted by the cloud provider to restart, NOT to fix broken code. [1]
    :param incrementor: This class will print a status method for every :param:incrementor jobs completed with a
        percentage of total jobs completed. [500]
    :param log_update_time: How often should the log of current jobs be printed in seconds. [60]
    :param download_on_complete: Should ALL file outputs be downloaded on subjob completion? Setting this
        option to 'True' will download all files to the current instance and provide a :func:Path. If 'False'
        (default), the value in the output dictionary will be a :func:dxpy.dxlink(). [False]
    """

    def __init__(self, concurrent_job_limit: int = 100, retries: int = 1, incrementor: int = 500,
                 log_update_time: int = 60, download_on_complete: bool = False):

        self._logger = MRCLogger(__name__).get_logger()

        # Dereference class parameters
        self._concurrent_job_limit = concurrent_job_limit
        self._incrementor = incrementor
        self._log_update_time = log_update_time
        self._download_on_complete = download_on_complete

        # We define three difference queues for use during runtime:
        # job_queue   – jobs waiting to be submitted
        # job_running – jobs currently running, with a dict keyed on the DX job-id and with a value of DXJobDict class
        #   which contains the job class from instantiation and information about the job
        #   and information about the job
        # job_failed  – A list of jobs which failed during runtime which, if the job has additional retries, can
        #   be resubmitted
        self._job_queue: List[DXJobInfo] = []
        self._job_running: Dict[str, DXJobDict] = dict()
        self._job_failed: List[DXJobInfo] = []

        # Job type & count monitoring
        self._queue_type: Optional[Environment] = None
        self._queue_closed = False
        self._total_jobs = 0
        self._num_completed_jobs = 0

        # Set default job instance type
        self._retries = retries
        if 'DX_JOB_ID' in os.environ:
            parent_job = dxpy.DXJob(dxid=os.getenv('DX_JOB_ID'))
            self._default_instance_type = \
            parent_job.describe(fields={'systemRequirements': True})['systemRequirements']['*']['instanceType']
        else:
            self._default_instance_type = None

        # Manage returned outputs
        self._output_array = []

    def __iter__(self) -> Iterator[Dict[str, Any]]:
        """Return an iterator over the outputs collected when jobs finish.

        Outputs returned by the class are a list of dictionaries formatted like::

            # Top-level list
            [
                # Per-job output lists
                [  # job1
                    # output dictionaries
                    {
                    'output1': output_value,
                    'output2': output_value,
                    'outputN': output_value
                    }
                ],
                [  # job2
                    {
                    'output1': output_value,
                    'output2': output_value,
                    'outputN': output_value
                    }
                ]
            ]

        where dict keys are identical to those passed to the :func:`launch_applet()` / :func:`launch_job()` 'outputs'
        parameter and where 'output_value' is either the actual output (e.g., str, int, boolean), if NOT a file,
        or if a file, a dxpy.DXLink() reference to a file on the remote file system, OR pathlib.Path(s) to the
        locally downloaded output itself if download_on_complete is True.

        :return: An iterator of output references
        """
        return iter(self._output_array)

    def __len__(self) -> int:
        """Returns the number of outputs currently in the output queue

        :return: The number of outputs currently in the output queue
        """
        return len(self._output_array)

    def launch_applet(self, applet_hash: str, inputs: Dict[str, Any], outputs: List[str] = None,
                      destination: str = None, instance_type: str = None, name: str = None) -> None:
        """Launch a DNANexus job with the given parameters from a LOCAL machine.

        The only required parameters for this function are :param:applet_hash and :param:inputs. This method will add
        a job with input parameters provided by this method call to the class :param:self._job_queue.

        DO NOT use this method if operating within a current DNANexus job. There may be unforeseen consequences...

        :param applet_hash: The applet hash for the applet that should be run (e.g., applet-1234567890ABCDEFGabcdefg)
        :param inputs: The function inputs. These must include all default inputs and be identical to those defined
            in the dxapp.json inputs section.
        :param outputs: The function outputs. These must include all default outputs and be identical to those defined
            in the dxapp.json outputs section. May be 'None' if there are no defined outputs from the given
            applet. [None]
        :param destination: Where should outputs be placed on the DNANexus platform. Default places outputs in the root
            level directory for the executing project (e.g., '/'). [None]
        :param instance_type: What instance type should be used? Default sets the instance type based on the
            'instance_type' specification in the dxapp.json. [None]
        :param name: Name of the job. Default names the job after the executing applet name. [None]
        """

        # Check if the queue has been closed by submit_queue()
        if self._queue_closed is True:
            raise dxpy.AppError('Cannot submit new subjobs after calling monitor_subjobs()!')

        # Make sure only identical job types have been launched
        if self._queue_type is Environment.DX:
            raise dxpy.AppError('Cannot mix jobtypes between launch_applet() and launch_job()!')
        elif self._queue_type is None:
            self._queue_type = Environment.LOCAL

        self._total_jobs += 1

        # Lists are immutable, so if None is provided, set to empty
        if outputs is None:
            outputs: List[str] = []

        input_parameters: DXJobInfo = {'function': applet_hash,
                                       'properties': {},
                                       'input': inputs,
                                       'outputs': outputs,
                                       'job_type': Environment.LOCAL,
                                       'retries': 0,
                                       'destination': f'/{destination}',
                                       'name': f'subjob_{self._total_jobs}' if name is None else None,
                                       'instance_type': instance_type if instance_type else self._default_instance_type}

        self._job_queue.append(input_parameters)

    def launch_job(self, function: Callable, inputs: Dict[str, Any], outputs: List[str] = None,
                   instance_type: str = None, name: str = None) -> None:
        """Launch a DNANexus job with the given parameters from a REMOTE machine.

        The only required parameters for this function are :param:function and :param:inputs. This method will add
        a job with input parameters provided by this method call to the class :param:self._job_queue.

        There is an entire logic for why we pass the function to the method rather than the string representation of
        the method's name:

        1. To be able to 'see' the methods of other files / packages decorated with dxpy.entry_point(), the class has
        to be imported into the calling file (e.g. 'import from'). Using the string representation makes the python
        interpreter / pycharm think that the import isn't used, so we use the actual function.

        2. We then convert to the string representation below because the DNANexus DXJob call requires a string
        representation.

        3. This also allows us to use the 'inspect' package to set a DNANexus 'property' for this job that is equal
        to the name of the containing python package. This is so that we can find this corresponding package to
        import when the subjob is launched. For more information about this, see the :func:check_subjob_decorator()
        method.

        DO NOT use this method if operating from a local machine. There may be unforeseen consequences...

        :param function: A python function (Callable type). This function MUST be decorated with the dxpy.entry_point()
            decorator, with the only parameter to this decorator being the name of the function. This name must be
            identical to the def for that function (e.g., dxpy.entry_point('test'); def test():)
        :param inputs: The function inputs. These must include all default inputs and be identical to those defined
            in call for :param:function. Only json serializable types can be included (python primitives, dict, and
            list).
        :param outputs: The function outputs. These must include all default outputs and be identical to those returned
            by the function's output. This output MUST be in the form of a named dictionary
            (e.g., outputs = {'output': 5}. May be 'None' if there are no defined outputs from the given applet. [None]
        :param instance_type: What instance type should be used? Default sets the instance type based on the
            'instance_type' specification in the dxapp.json. [None]
        :param name: Name of the job. Default names the job after the executing applet and :param:function name. [None]
        """

        # Check if the queue has been closed by submit_queue()
        if self._queue_closed is True:
            raise dxpy.AppError('Cannot submit new subjobs after calling monitor_subjobs()!')

        # Make sure only identical job types have been launched
        if self._queue_type is Environment.LOCAL:
            raise dxpy.AppError('Cannot mix jobtypes between launch_applet() and launch_job()!')
        elif self._queue_type is None:
            self._queue_type = Environment.DX

        self._total_jobs += 1

        # Lists are immutable, so if None is provided, set to empty
        if outputs is None:
            outputs: List[str] = []

        input_parameters: DXJobInfo = {'function': function.__name__,
                                       'properties': {'module': inspect.getmodule(function).__name__},
                                       'input': inputs,
                                       'outputs': outputs,
                                       'job_type': Environment.DX,
                                       'retries': 0,
                                       'destination': None,
                                       'name': None if name is None else name,
                                       'instance_type': instance_type if instance_type else self._default_instance_type}

        self._job_queue.append(input_parameters)

    def submit_queue(self) -> None:
        """Submit the job queue created by adding jobs to launch_applet / launch_job and wait for submitted jobs to
        finish executing.

        This method submits all jobs added to self._job_queue to the DNANexus job scheduler. It also manages watching
        jobs and checking when they complete or fail. To note: when subjob spot instances generated from a currently
        running DNANexus job fail due to a SpotInstanceInterruption, they bypass this checking process and will
        always fail the parent process, which effectively bypasses the retries parameter defined in this class.

        Importantly, when all jobs finish, this method waits until the next iteration period as defined by
        log_update_time parameter provided to the class constructor; therefore, it is best to avoid long update times.
        """

        # Close the queue to future job submissions to save my sanity for weird edge cases
        self._queue_closed = True

        self._logger.info("{0:65}: {val}".format("Total number of jobs to iterate through", val=self._total_jobs))

        # These variables are used to keep track of time so that we can print a job log at the requested interval,
        # independently of the time it takes to monitor jobs.
        last_time = time()
        last_log_time = 0

        # Keep going until we get every job submitted or finished...
        while len(self._job_queue) > 0 or len(self._job_running.keys()) > 0:

            # We only want to print the status when the log_update_time has passed. To do this we keep track of the time
            # passed since the last iteration and add it to last_log_time. If last_log_time is greater than
            # self._log_update_time, then we print the log and reset last_log_time to 0.
            current_time = time()
            last_log_time += current_time - last_time
            last_time = current_time
            if last_log_time >= self._log_update_time:
                last_log_time = 0
                self._print_status()

            self._monitor_subjobs()  # Iterate through all jobs in the queue or currently running
            if len(self._job_running.keys()) > 0:
                sleep(30)

        if len(self._job_failed) > 0:
            self._logger.info('All jobs completed, printing failed jobs...')
            for failed_job in self._job_failed:
                self._logger.error(f'FAILED: {failed_job}')
        else:
            self._logger.info('All jobs completed, No failed jobs...')

    def _print_status(self) -> None:
        """Print a time-stamped log of jobs waiting in the queue to be submitted, currently running jobs, and failed
        jobs.
        """
        self._logger.info(f'{"Jobs currently in the queue":{65}}: {len(self._job_queue)}')
        self._logger.info(f'{"Jobs currently running":{65}}: {len(self._job_running.keys())}')
        self._logger.info(f'{"Jobs failed":{65}}: {len(self._job_failed)}')

        curr_time = datetime.today()
        self._logger.info(f'{curr_time.isoformat("|", "seconds"):{"-"}^{65}}')

    def _monitor_subjobs(self) -> None:
        """This method is the primary monitoring point for queued, submitted, and finished jobs.

        This method will first check to see if there are any jobs in the job queue, and if the number of currently
        running jobs is less than self._concurrent_job_limit, will attempt to submit new jobs according to the type of
        job (:func:Environment).

        If there are no more jobs to submit, it will then automatically monitor submitted jobs for a completed
        :func:JobStatus.
        """

        # Set a boolean for allowing to submit jobs UNTIL we hit the job limit (defined by self._concurrent_job_limit)
        can_submit = True

        # Only run the submission process while jobs are still in the queue
        while len(self._job_queue) > 0:

            job = self._job_queue.pop()

            # Make sure the queue isn't full...
            # If it is, go to sleep for 60s and check again
            while can_submit is False:
                if len(self._job_running.keys()) < self._concurrent_job_limit:
                    can_submit = True

                self._print_status()
                self._monitor_submitted()
                sleep(60)

            # A bit strange, but the Environment enum's value is a class that we can instantiate for our specific job
            # type
            if job['job_type'] == Environment.DX:
                dxjob = job['job_type'].value()
                dxjob.new(fn_input=job['input'], fn_name=job['function'], instance_type=job['instance_type'],
                          properties=job['properties'], name=job['name'])

            elif job['job_type'] == Environment.LOCAL:
                dxapplet = job['job_type'].value(job['function'])
                dxjob = dxapplet.run(applet_input=job['input'], folder=job['destination'], name=job['name'],
                                     instance_type=job['instance_type'], priority='low')

            else:
                raise RuntimeError('Job does not have type DX or LOCAL, which should be impossible')

            # Build a dict that contains all information necessary to monitor AND resubmit this job (if necessary)
            self._job_running[dxjob.describe(fields={'id': True})['id']] = {'job_class': dxjob,
                                                                            'job_info': job}

            # Lock the submission process until we have space to launch additional jobs
            if len(self._job_running.keys()) >= self._concurrent_job_limit:
                can_submit = False

        self._monitor_submitted()

    def _check_job_status(self, job: DXJobDict) -> RunningStatus:
        """A helper function that checks if a job is finished and collects outputs from finished jobs.

        This method first generates a JobStatus Enum by checking the current DNANexus job status using the DXJob
        object stored in the DXJobDict object by calling :func:dxpy.DXJob.describe(). This JobStatus is then
        translated into a RunningStatus Enum which codes for only Complete / Running / Failed jobs to simplify the
        decision process of deciding the end-point of a given job. Finally, a job is RunningStatus.COMPLETE,
        we use the DXJobDict-defined outputs to retrieve :func:dxpy.dxlink() for those outputs and add them to
        self._output_array.

        if self._download_on_complete is True, we also download all files directly to

        To be clear, this method could be contained within :func:self._monitor_submitted(), but was easier to follow
        when abstracted into a separate method.

        :param job: A DXJobDict TypedDict object containing information about a job.
        :return: A :func:RunningStatus enum indicating the status of the current job.
        """

        description = job['job_class'].describe(fields={'state': True})
        curr_status = JobStatus[description['state'].rstrip().upper()]
        if curr_status.value == RunningStatus.COMPLETE:
            output_dict = {}
            if len(job['job_info']['outputs']) > 0:
                # Do a describe() call here, so we only have to do it once to save time
                output_values = job['job_class'].describe()['output']
                for output in job['job_info']['outputs']:

                    output_ref = job['job_class'].get_output_ref(output)
                    output_key = output_ref['$dnanexus_link']['field']
                    output_value = output_values[output_key]

                    # Need to check if they are files...
                    # This is if the output is a list
                    if type(output_value) is list:
                        new_values = []
                        for value in output_value:
                            # This is possibly (likely) a file
                            if '$dnanexus_link' in value:
                                if value['$dnanexus_link'].startswith('file-'):
                                    if self._download_on_complete:  # Download the file if the user wants it locally
                                        new_values.append(download_dxfile_by_name(value, print_status=False))
                                    else:
                                        new_values.append(value)
                                else:
                                    new_values.append(value)

                            # This is unlikely to be a file
                            else:
                                new_values.append(value)

                        output_dict[output_key] = new_values

                    # This is if the output is just a single value
                    else:
                        # Dictionaries are possibly (likely) a file.
                        # In fact – I don't think dictionaries can happen here unless they are files, but adding
                        # else statements just to make sure.
                        if type(output_value) is dict:
                            if '$dnanexus_link' in output_value:
                                # This is still likely a file...
                                if output_value['$dnanexus_link'].startswith('file-'):
                                    if self._download_on_complete:  # Download the file if the user wants it locally
                                        output_dict[output_key] = download_dxfile_by_name(output_value,
                                                                                          print_status=False)
                                    else:
                                        output_dict[output_key] = output_value
                                else:  # This is something else that I don't think actually exists in DNANexus...
                                    output_dict[output_key] = output_value
                            else:  # I don't think this else can happen, but adding it just to be sure
                                output_dict[output_key] = output_value
                        # This is unlikely to be a file
                        else:
                            output_dict[output_key] = output_value

            self._output_array.append(output_dict)

            self._num_completed_jobs += 1
            if math.remainder(self._num_completed_jobs, self._incrementor) == 0:
                self._logger.info(
                    f'{"Total number of jobs finished":{65}}: {self._num_completed_jobs} / {self._total_jobs} '
                    f'({((self._num_completed_jobs / self._total_jobs) * 100):0.2f}%)')

        return curr_status.value

    def _monitor_submitted(self) -> None:
        """Iterate through all currently running jobs and decide if they are finished, complete, or failed.

        This method wraps :func:self._check_job_status(). It takes the output of that function and decides whether
        to, if:

        * Completed: remove the (completed) job from the job_running dict

        * Failed: Remove the failed job from the job_running_dict and, if the job has retries, add the job pack to
        self._job_queue

        * Running: Continue to monitor the job until completed
        """

        curr_keys = list(self._job_running.keys())
        for job_id in curr_keys:
            job_status = self._check_job_status(self._job_running[job_id])
            if job_status is RunningStatus.COMPLETE:
                del self._job_running[job_id]
            elif job_status is RunningStatus.FAILED:
                job = self._job_running[job_id]['job_info']
                del self._job_running[job_id]
                if job['retries'] < self._retries:
                    job['retries'] += 1
                    self._job_queue.append(job)
                else:
                    self._job_failed.append(job)


def check_subjob_decorator() -> Optional[str]:
    """This class checks to see if the dx Applet is actually a subjob being run on the DNANexus platform.

    To be able to run subjobs on DNANexus, the class / method MUST be imported using standard python imports (e.g.,
    import ... from ...) by the instantiating file that contains the dxpy.entry_point('main') decorator. This means
    that modules (e.g., 'burden') that are dynamically loaded WILL NOT be found in the python classpath prior to
    DNANexus attempting to launch the subjob with the indicated dxpy.entry_point() decorator. To solve this, we do two
    things:

    1. For all subjobs being launched, we add a property to the job indicating where this method
    (check_subjob_decorator) should look for the decorated method (see DXJobInfo in this file for more information).

    2. When the new subjob is launched, we then run this method before dxpy.run() is called to make sure that the
    decorated method is properly included in the dxpy.utils.exec_utils.ENTRY_POINT_TABLE dict that tells the DNANexus
    job handler what function to run at startup. This approach uses the import_module function from importlib to
    dynamically load the requested methods into the python classpath

    :return: The name of the identified module that was loaded by this method or None if no module was loaded
    """

    loaded_module = None
    job = dxpy.DXJob(dxpy.JOB_ID)
    if 'module' in job.describe()['properties']:
        loaded_module = job.describe()['properties']['module']
        try:
            import_module(loaded_module)
        except ModuleNotFoundError:
            raise

    return loaded_module


def prep_current_image(required_files: List[dict]) -> CommandExecutor:
    """A helper function for launched subjobs that automatically downloads the required Docker image and downloads
    any requisite files

    :param required_files: A list of :func:`dxpy.dxlink()` dictionaries of files to download
    :return: The CommandExecutor for running jobs on the instance
    """

    cmd_executor = build_default_command_executor()

    for file in required_files:
        download_dxfile_by_name(file, print_status=False)

    return cmd_executor
